---
title: Automated Testing
authors:
  - name: Eric Terry
    email: Eric_Terry@McAfee.com
  - name: Nathan Bitikofer
    email: Nathan_Bitikofer@McAfee.com
  - name: Matt Meng
    email: matt.meng@intel.com
  - name: Matt Hulse
    email: Matt_Hulse@McAfee.com
layout: page
---

:page-layout: base
:toc: right
:icons: font
:idprefix:
:idseparator: -
:sectanchors:
:source-highlighter: highlight.js
:mdash: &#8212;
:language: asciidoc
:source-language: {language}
:table-caption!:
:example-caption!:
:figure-caption!:
:linkattrs:

[NOTE]
.This document is under active development and discussion!
If you find errors or omissions in this document, please let us know. You can ask questions and discuss anything on Chat or Email.

== Introduction

Welcome to the System Integration Team's (SIT) documentation page for automated testing!  Here you will find all the information you need to get started writing tests for the SIEM, complete with concepts, examples, http://automation.ida.lab:1234/docs/screencasts/file/docs/contents.md[icon:video-camera[] screencasts^], and tips.  For detailed documentation on our gems, see:  http://automation.ida.lab:1234

.SIEM testing history
****
mailto://Matt_Hulse@McAfee.com[Matt Hulse] created the first https://www.ruby-lang.org[Ruby^] testing tool here back in 2009.  *NitroDB* was a Ruby Gem that could interact with the database.  A couple years later in 2011, he wrote *NitroAPI* in Ruby, which could call API's.  This was the beginning of automated testing in Ruby for the SIEM.

http://whiteboard.ida.lab[Whiteboard^] was born early in 2012 by mailto://Kelly_Jensen.McAfee.com[Kelly Jensen].  It's purpose initially was only for ISOing and setting up SIEM environments using the *NitroAPI* gem Matt wrote.

In Fall 2012, mailto://Thomas_Constantino@McAfee.com[Tom Constantino] (the QA Manager at the time) gave Kelly the task of creating a framework for running automated tests.  This was a high priority task and time was limited.  https://github.com/ruby/rake[Rake^], a make-like build utility for Ruby, seemed like the best tool to run these tests at the time.  A *Test Runner* was created which could run these *Rake* tests.  All of the QA department then jumped onboard to learn Ruby and start writing automation.  *Whiteboard* morphed into not only setting up environments, but now running tests against them and generating reports and emails.

Eventually a dedicated *Automation Team* was created in September 2013. Matt Hulse was chosen as the manager.  mailto://Jefferson_Dewey@McAfee.com[JT Dewey] was hired straight into this team early in 2014 as the first team member.  Kelly officially transitioned to this team shortly thereafter.  mailto://Eric_Terry@McAfee.com[Eric Terry] was the third addition, coming from the QA Department.

In January 2014 mailto://Matt_Meng.McAfee.com[Matt Meng] from the Middleware Department decided he wanted a _sexier_ testing framework for his API tests he was writing.  *M3* was born (a custom in-house testing framework). *M3* became the new standard for writing tests.  Many improvements came from *M3* and development was ongoing.

The *Automation Team* continued development on *M3* and many other testing tools.  Interns would come and go every semester, helping our team with writing tests and developing the automation frameworks and tools.

In March 2015 the *Automation Team* became the *System Integration Team* as part of our transition to Scrum.  That is when mailto://Stuart_Purser@McAfee.com[Stuart Purser], mailto://Matt_Meng.McAfee.com[Matt Meng], and mailto://Scott_Southwick@McAfee.com[Scott Southwick] came onboard, with mailto://Steven_Ridges@McAfee.com[Steve Ridges] as the new manager.

A third (and last) testing framework was agreed upon in May 2015 that would not only provide testing using API's, SQL queries, and SSH commands; but also provide testing the new GUI for the SIEM.  http://rspec.info[RSpec^] can do it all.
****

=== Why write automated tests?

Those who have been in QA know well the pains and struggles that come with manual testing.  Sure the first time around is no big deal.  Test the feature.  Done.  But when it comes to releasing another version of the SIEM, and they find out they have to "sign off" on all of their "areas", it becomes a giant beast.  Now they have to redo their effort of manually testing a feature that was completed months ago.  As time goes on, their "area" expands and increases, with ever more features to manually test.  There comes a point where it is impossible to manually test all areas of the product before a release.  The technical debt of not having automated regression tests for all of those features becomes horrendous.  As a result, quality suffers and confidence in a working product goes down the drain.

This is why we need machines to do all this testing for us.  Machines are much more capable than humans at consistently testing an area of the product the same way each time, and every time.  Machines can run thousands of regression tests multiple times a day, providing everyone with confidence and trust in the product and every single commit into the code repository.  The more automation that we have, the higher the quality of our product becomes and the more time developers have for doing what they love doing.

=== Why Ruby?

TIP: Learn Ruby in less than 9 hours by going through the Ruby course at http://www.codecademy.com/tracks/ruby.  You can also try Ruby out interactively at http://tryruby.org.

"Ruby has just a deep emotional appeal of how beautiful you can write something."
-- David Heinemeier Hansson, Rails Creator David Heinemeier Hansson Explains Why He Loves Ruby, August 2, 2010

"Like most things in life, choosing the correct tool for the job needs some careful consideration and planning. Ruby makes a lot of sense for getting applications off the ground quickly and reinforcing good practices like testing, code separation, and readability that I find important in forming new digital humanities programmers."
-- Wayne Graham, Why Ruby?, May 11, 2010

"Ruby is indeed evil. So evil. Extremely evil. But why so evil? Because it's so beautiful. The syntax is so elegant, everything are objects. Everything makes sense. The Ruby-way of doing things is so sexy."
-- Simon Eskidsen, What I wish a Ruby programmer had told me one year ago, April 2010

== Types of tests

There are three main types of tests.  Unit tests, system tests, and end-to-end (e2e) tests.

=== Unit tests

A true, pure unit test:

* Doesn't require a running system of the product
* Is written in the same language as the source code
* Run extremely fast
* Directly interacts with methods/functions/objects of the code
* Only tests certain pieces and parts of the product at certain levels

Unit tests are the first level of defensive.  In a http://www.thoughtworks.com/continuous-integration[Continuous Integration (CI)] environment, every commit to the source core repository runs these tests.  Feedback can immediately be provided back to the developer on the results of these tests.  Unit tests run on Jenkins and have the power to fail the build.

=== System tests

WARNING: These types of tests are gradually being phased out in favor of pure unit tests and end-to-end tests.  This is because of maintenance costs associated with system tests.

A system test:

* Requires a running system of the product
* Is language agnostic (we use Ruby)
* Runs slower than unit tests
* Indirectly tests the code usually via the API
* Does not test the complete system from backend through middleware to the frontend

System tests have been the main type of automated testing here for the SIEM, usually by directly interacting with the API.  These tests can also interact directly with the command line (via SSH) and the database (via SQL).  Wrappers have been created for the API that makes it easier to interact with the API using Ruby.  The cost of keeping these wrappers (and thus helpers and tests) up to date with the product is high.

=== End-to-end tests

CAUTION: Currently we can only test the new GUI with end-to-end tests.

An end-to-end (e2e) test:

* Requires a running system of the complete product
* Is language agnostic (we use Ruby)
* Runs slower than system tests
* Directly interacts with the frontend GUI via a browser
* Tests the complete product from the frontend through middleware to the backend and back up

End-to-end tests are not possible with a Flash frontend.  This is why we have never written these types of tests here until now.  With the new GUI for the SIEM, this now becomes possible.  E2e tests simulate key presses and mouse clicks on an actual Internet browser.

== Levels of testing

There are four levels of testing that we endorse and support here.  These levels of tests are only applicable to system tests and e2e tests, not unit tests.

[NOTE]
====
.icon:clock-o[] How long should a test generally take to run?
* *BVT* - Less than 1 minute
* *Basic* - Less than 5 minutes
* *Acceptance* - Between 5 and 20 minutes
* *Comprehensive* - Longer than 20 minutes
====

=== Build verification tests

Build Verification Tests, or BVT's for short, are very fast running tests that make sure the build is good and testable.  If any of the BVT tests fail, the entire test run is terminated.  Basic, Acceptance, and Comprehensive tests will not run.

"Running a BVT before initiating a full test run is important because it lets developers know right away if there is a serious problem with the build, and they save the test team wasted time and frustration by avoiding test of an unstable build."
-- Wikipedia, Build verification test

BVT's should not test specific features in the product, but rather make sure core functionality is working enough for the rest of the tests to even be worth running.  They should be extremely stable and always have known expected results.

=== Basic tests

TIP: When a feature is automated, you should consider writing more than just one test for it.  It is often helpful to write a Basic, Acceptance, and Comprehensive test for it.

A basic test is one that tests a feature or area of the product, but doesn't go into a lot of detail.  These should be quick running tests that do a "quick pass" to make sure core functionality of the feature is working properly.  For instance, a basic test could test one scenario with default options (even though the feature has many scenarios).

As an example, if you are testing a form on a web page, you would make sure you can fill out the form with valid data and submit it and get a proper response.  An acceptance or comprehensive test could run a similar test, but maybe enable a checkbox, fill in optional input boxes, and try filling out the form with invalid data.

=== Acceptance tests

This is the most common level of testing that we have here.  An acceptance test should verify that the feature works as intended for _most_ scenarios.  This is a higher detail level of testing than Basic tests, but not as detailed as Comprehensive tests.

TIP: It is recommended that you write your test before developing the feature.  This is known as Test Driven Development.  But only do this if it makes sense for you and the situation.

=== Comprehensive tests

A comprehensive test usually takes longer than 20 minutes to run and tests most of the scenarios of a feature.  They should include many failure scenarios as well (tests that make sure the system fails properly when something invalid is performed).  Comprehensive tests could also use the concept of data-driven testing, where the variety of inputs to a test would be in a file or an array and looped through.

[NOTE]
====
.icon:table[] Data-driving testing
"Anything that has a potential to change (also called variability, and includes elements such as environment, end points, test data, locations, etc.) is separated out from the test logic (scripts) and moved into an 'external asset'. This can be a configuration or test dataset. The logic executed in the script is dictated by the data values."
-- Wikipedia, Data-driven testing
====

== Helpers and wrappers

System tests have the concept of helpers and wrappers.  Systems tests typically use the API to interact with the product.  Because interacting directly with the raw API is very messy, we use wrappers.  Because there is common functionality we want to share between tests, we use helpers.

TIP: If you cannot play the videos, try using Chrome or try right clicking on the video and selecting "View Video" or "Open video in new tab".

video::http://automation.ida.lab:8000/mp4/test_types_helpers_and_wrappers.mp4[width=500, title="Helpers and wrappers", poster="http://automation.ida.lab:8000/thumbnails/test_types_helpers_and_wrappers.jpg"]

=== Wrappers

TIP: View detailed documentation for all wrappers at http://automation.ida.lab:1234/docs/wrappers/frames

video::http://automation.ida.lab:8000/mp4/rspec_wrappers.mp4[width=500, title="Using wrappers in Rspec", poster="http://automation.ida.lab:8000/thumbnails/rspec_wrappers.jpg"]

API wrappers provide a much friendlier interface to the raw API.  They convert ruby-ized values and types to what the raw API request requires.  They also take the raw response from the API and convert it to ruby-ized values.

This makes it so that test writers don't have to worry about DC1 and DC2 lists, CSV lists, T and F booleans, 0 and 1 booleans, yes and no booleans, nested arrays and hashes in the raw API format, strings vs integers, etc.  A test writer would just have to pass in a boolean "true" and the wrapper would automatically convert it to T, 1, or yes depending on the specified type for the argument in the wrapper. So a test would use normal ruby Fixnums, Strings, Hashes, Arrays, and Booleans for all arguments passed to an API wrapper.

video::http://automation.ida.lab:8000/mp4/my_first_m3_wrapper.mp4[width=500, title="My first wrapper", poster="http://automation.ida.lab:8000/thumbnails/my_first_m3_wrapper.jpg"]

video::http://automation.ida.lab:8000/mp4/advanced_m3_wrappers.mp4[width=500, title="Advanced wrappers", poster="http://automation.ida.lab:8000/thumbnails/advanced_m3_wrappers.jpg"]

For detailed documentation on how to create/modify wrappers, see:  http://automation.ida.lab:1234/docs/m3/file/docs/wrappers.md

=== Helpers

TIP: View detailed documentation for all helpers at http://automation.ida.lab:1234/docs/helpers/frames

Helpers provide a way of putting commonly used functionality into a common location which can be accessed in a test.

video::http://automation.ida.lab:8000/mp4/my_first_m3_helper.mp4[width=500, title="My first helper", poster="http://automation.ida.lab:8000/thumbnails/my_first_m3_helper.jpg"]

System helpers usually should be designed to represent an object in the product.  For example, there could be a helper created for Users.  This helper would have methods defined such as create, delete, edit, assign_group, change_password, etc.  Many tests would want to use a Users helper to create a test user before testing the product.  Requiring every test writer to know the exact API calls and parameters needed to add a user would be time consuming and error-prone.  Helpers provide a higher level for a test writer to perform certain actions.

Let's say that adding a user took 6 API calls that each took many parameters.  If this code was _copied and pasted_ in 30 tests, and one of the API parameters changed in middleware, it would required fixing 30 tests!  However, if those 6 API calls were in a helper method and those 30 tests all used that common helper, then the change would only have to be applied once in only 1 file.

"The developer who learns to recognize duplication, and understands how to eliminate it through appropriate practice and proper abstraction, can produce much cleaner code than one who continuously infects the application with unnecessary repetition."
-- Steve Smith, Don't Repeat Yourself (DRY)

Another use case for helpers is when, as a test writer, you want to test a specific functionality, but in order to get the product in a state where that functionality is testable, many things have to be setup.  You shouldn't have to care or know _how_ to get the product in that state.  After all, that is not the _point_ of the test!  If many helpers are created and well documented, a test writer could just use a helper or two to get the box in the correct state.  After in the correct state, the test writer could then write the actual test they cared about in the first place.

video::http://automation.ida.lab:8000/mp4/rspec_sys_helpers.mp4[width=500, title="Using helpers in Rspec", poster="http://automation.ida.lab:8000/thumbnails/rspec_sys_helpers.jpg"]

== Test structure

NOTE: http://rspec.info/[RSpec^] is the open-source testing framework we use for all _system tests_ and _e2e tests_.  In this section, we will be talking about the general principles of a test structure.  Further below in this document are specifics of the test structure according to RSpec.

=== Setup

Use the concept of a *setup* to create resources the test needs to correctly test a given feature. For example: if the test requires certain users, data sources, or other settings to be enabled or disabled; then the test creates those resources in the setup. Setup is not the place to do any testing. This means that there shouldn't be any asserts.

You can have one setup for the entire test file and/or multiple setups for certain groups of test cases.  You can also have a setup that runs before every test case.

NOTE: RSpec uses `before :all` blocks for a setup that runs once before all tests cases and `before :each` blocks for a setup that runs before every test case.

=== Teardown

The concept of *teardown* can be used in a test to remove the resources that were *setup* as well as anything that was created during a test case. This is referred to as "cleaning up after yourself" or "Leave No Trace". For example, if I create a user called _user1_ in the setup, my teardown should remove that user. If not, then the next test might fail because it is not expecting a _user1_ to already exist. You can also have a teardown that runs after every test case.

TIP: You can use the *Snapshot* helper to revert the environment back to how it was before you ran your test.  This makes tearing down your test much easier.  icon:video-camera[]  http://automation.ida.lab:8000/mp4/snapshot_create_and_revert.mp4[Snapshots^]

NOTE: RSpec uses `after :all` blocks for a teardown that runs once after all tests cases and `after :each` blocks for a teardown that runs after every test case.

=== Test case

IMPORTANT: A test case dependency occurs when a test case uses something that was created in a previous test case. Most commonly that something is a member/instance variable. Member variables are variables that are preceded with an @ symbol in ruby. Test case dependencies are a bad practice. If member variables are used, they should be declared and initialized in the setup, not in a test case. This is the reason why: there is a good possibility that the test case will error out before that variable is set. If that variable doesn't get set it will cause the next test case to fail, because it was "depending" on that variable. The main idea here is that test cases should be 100% independent from any other test case. This means that all cases should be able to run in _any order_ and _by itself_.

A test case should be independent of any other test case.  There should also be only one assert per test case.  This makes it easier to figure out what went wrong and why when a test case fails.  A good methodology to use when writing a test case is *Arrange, Act, Assert*.  This helps organize a test case and makes sure you are only testing one thing per test case.  Read more about it here:  http://www.arrangeactassert.com/why-and-what-is-arrange-act-assert/

[source,ruby]
----
describe 'Math' do
  before( :all ) { @calculator = Calculator.new }

  context 'Addition' do
    # Arrange // <1>
    let( :first_number ) { 5 }
    let( :second_number ) { 3 }

    # Act // <2>
    subject { @calculator.add( first_number, second_number ) }

    # Assert // <3>
    it { is_expected.to eq 8 }
  end
end
----
<1> *Arrange* all necessary preconditions and inputs.
<2> *Act* on the object or method under test.
<3> *Assert* that the expected results have occurred.

== Setup environment

NOTE: For those who want to install Ruby on Windows, check out https://bugzilla.ida.lab/wiki/index.php/Installing_M3_on_Windows_--_A_Guide_for_the_Adventurous[Installing M3 on Windows -- A Guide for the Adventurous].

=== Install Ruby

There are several ways to install Ruby.  Below is a video describing the way to install Ruby using the https://rvm.io[Ruby Version Manager (RVM)] on Linux.  Whatever method you use, make sure you install at least version 2.1.

video::http://automation.ida.lab:8000/mp4/setup_environment_install_ruby.mp4[width=500, title="Install Ruby", poster="http://automation.ida.lab:8000/thumbnails/setup_environment_install_ruby.jpg"]

NOTE:  If the command to download the key using gpg fails because of a blocked port issue on the network, try running `gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3` instead.

WARNING:  If `bundle` is not installed on your system (or if it prompts you to attempt to install it via apt-get), run `gem install bundler`.  Do not run `sudo apt-get install bundler`!

=== Install RSpec

To install RSpec (and all dependencies needed to run automation) type `bundle` within the `core/automation` directory.

=== Get libNitroDB.so

In order to interact with the database and make SQL calls, you will need libNitroDB.so on your computer.  If you don't have it on your computer or if it is in the wrong location, you will get an error similar to:

----
Could not open library 'NitroDB': NitroDB: cannot open shared object file: No such file or directory.
Could not open library 'libNitroDB.so': libNitroDB.so: cannot open shared object file: No such file or directory
/var/lib/gems/2.1.0/gems/nitrodb-1.0.0/lib/nitrodb.rb:278:in `nitro_get_buildstamp': undefined method `Nitro_GetBuild' for EDB:Module (NoMethodError)
----

* Navigate to `\\idastna1.corp.nai.org\odin\NitroEDB\nightly_build`
* Open the latest directory that makes sense for your machine (Linux/Win and 32/64 bit)
* Copy `libNitroDB.so`
* Place the file in the `lib` directory that Ruby has access to

TIP: If you installed Ruby using RVM as shown in the video above, you can place the `libNitroDB.so` file in `~/.rvm/rubies/ruby-[VERSION]/lib/`.

=== Setup configuration file

The configuration file is at `~/.m3/config`. It is a JSON file that contains several options.

==== "devices"

This key specifies device information custom to your setup. It is the only required key. It lists all devices you would like to use for testing. The devices do not need to be active or reachable, this key simply lets you list out all the devices you _could_ use for testing.

[source,json]
----
"devices": {
  "esms": {
    <nickname>: {
      "address": <ip_address>
      "password": <password_of_root>
    },
    ...
  },
  ...
}
----

Under the *devices* hash, you can specify any device type of the following (as shown above):

* aces
* adms
* dems
* elms
* esms
* ipss
* recelms
* recs

Each device type is also a hash and can contain any number of devices. Each device requires a nickname used to refer to it, an IP address, and the root password. Example:

[source,json]
----
{
  "devices": {
    "esms": {
      "0": {
        "address": "10.75.80.200",
        "password": "Security.4u"
      }
    },
    "elms": {
      "1": {
        "address": "10.75.80.201",
        "password": "Security.4u"
      },
      "2": {
        "address": "10.75.80.202",
        "password": "Security.4u"
      }
    }
  }
}
----

.Custom SSH port
If you need to specify a custom SSH port, you can do so by providing a "ssh-port" key and value to the JSON object for the device like this:

[source,json]
----
{
  "devices": {
    "esms": {
      "0": {
        "address": "10.75.80.200",
        "password": "Security.4u",
        "ssh-port": 2222
      }
    }
  }
}
----

==== "loadouts"

CAUTION: Only use loadouts if your test needs more than one esm or more than one device type (like two aces, or two receivers, etc.).

This key specifies, per-test-unit, what devices to use for test units that have device loadouts. Devices can be referenced in the test units in the order in which they are given.

Available devices are ACEs, ADMs, DEMs, ELMs, ESM, IPSs, RECELMs, and RECs. Use the same devices keywords to reference each device type in the test unit loadout. The key for the device is the name of the device in the devices option.

[source,json]
----
"loadouts": [
  <test_unit_name>: {
    "esms": [
      <esm_1_name>,
      ...
    ],
    ...
  }
]
----

For example, given the following test unit:

[source,ruby]
----
loadout esms: 1, elms: 2
----

The test unit is asking for a single esm and two elms. If we don't fulfill this requirement, we can't run the test. We specify the devices to use with the following:

[source,json]
----
{
  "devices": {
      "esms": {
        "0": {
          "address": "10.75.80.200",
          "password": "Security.4u"
        }
      },
      "elms": {
        "1": {
          "address": "10.75.80.201",
          "password": "Security.4u"
        },
        "2": {
          "address": "10.75.80.202",
          "password": "Security.4u"
        }
      }
    }
  },
  "loadouts": [
    "my_test": {
      "esms": [
        "0"
      ],
      "elms": [
        "1",
        "2"
      ]
    }
  ]
}
----

== RSpec

TIP: For those who are familiar with the *M3* testing framework, please watch icon:video-camera[] http://automation.ida.lab:8000/mp4/from_m3_to_rspec.mp4[From M3 to Rspec^].

video::http://automation.ida.lab:8000/mp4/rspec_intro_training.mp4[width=500, title="Rspec intro training", poster="http://automation.ida.lab:8000/thumbnails/rspec_intro_training.jpg"]

From this point on, we will use RSpec terminology.

|===
|Term |Rspec Terminology

|Test Unit
|Specification (or Spec)

|Test Case
|Example

|Assertion
|Expectation
|===

All specs should be written and saved under the `automation/spec` directory.  The directory structure under `automation/spec` can be organized anyway that you feel is appropriate.  A standard may be determined later on.  All specs should have "_spec.rb" at the end of the file name for consistency and standard.

There are many ways to write an RSpec specification.  Because there are so many ways (and many are bad practice or deprecated), before reading anything else in this document, *please read the entire RSpec guidelines documentation at http://betterspecs.org/.*

For detailed documentation on RSpec Core, see http://www.relishapp.com/rspec/rspec-core/v/3-3/docs

=== How to run a spec

WARNING: Only run RSpec within the "automation" directory.  Running it anywhere else may have unexpected behavior.

To run a spec using the default device in your `~/.m3/config` file (which is the first listed ESM in the JSON):

----
cd core/automation
rspec spec/name_of_spec.rb
----

To run the demo_spec.rb (which will contain both passes and failures):

----
rspec spec/demo_spec.rb
----

To run a spec using a specific ESM that is defined withing your `~/.m3/config` file:

----
DEVICE=key_from_config rspec spec/demo_spec.rb
----

So if you had this as your config file:

[source,json]
----
{
  "devices": {
    "esms": {
      "110.135": {
        "address": "10.75.110.135",
        "password": "Q@_T3st.NGCP"
      }
    }
  }
}
----

You could run a spec like this:

----
DEVICE=110.135 rspec spec/demo_spec.rb
----

For all other ways to run RSpec, please run:

----
rspec --help
----

=== Structure

WARNING: Do not use the _should_ syntax! The _expect_ syntax is the new way of asserting in RSpec.  See http://betterspecs.org/#expect.

[source,ruby]
----
describe 'Login' do // <1>
  before( :all ) do // <2>
    @auth = Authentication.new( @esm )
  end

  before( :each ) do // <3>
    @auth.logout
  end

  context 'with no password' do // <4>
    # Arrange
    let( :user ) { 'NGCP' }
    let( :pass ) { '' }

    it 'raises a password required error' do // <5>
      # Act
      @auth.login( user, pass )

      # Assert
      expect { login }.to raise_error 'Password is required' // <6>
    end
  end

  context 'with no username' do
  end

  context 'with bad password' do
  end

  context 'with valid credentials' do
  end
end
----
<1> Name of the spec. In this case, the filename should be "login_spec.rb" because the spec is named "Login".
<2> This is the _setup_ for the entire spec. Anything that needs to be shared between examples can be set here. Usually the setup will create helper instances you will need throughout the spec.
<3> This is the _setup_ that will run before each example.
<4> Use _context_ blocks to make your test more clear and well organized.
<5> This is the actual example that contains the expectation.
<6> This is the expectation of the example. In this case, we are expecting that when we login with no password, the system should raise an error. `raise_error` is a built-in matcher.  There are many matchers provided by the RSpec framework.

video::http://automation.ida.lab:8000/mp4/rspec_example_test.mp4[width=500, title="Rspec example test", poster="http://automation.ida.lab:8000/thumbnails/rspec_example_test.jpg"]

=== Expectations

Expectations are where the actual testing takes place.  An expectation will always return a Boolean value (did it pass or did it fail?).  Best practice is to have only one expect per example.

.Basic structure
[source,ruby]
----
expect(actual).to matcher(expected)
expect(actual).not_to matcher(expected)
----

.Examples
[source,ruby]
----
expect(5).to eq(5)
expect(5).not_to eq(4)
----

=== Matchers

Matchers are different ways to make assertions.  What are you expecting a value to be, do, or have?  This is what matchers determine.

.A few examples
[source,ruby]
----
expect( result ).to eq 5 // <1>
expect( result ).to be String // <2>
expect( result ).to be > 3 // <3>
expect( result ).to be_between( 4, 9 ) // <4>
expect( result ).to match /^expression.*$/ // <5>
expect( result ).to be_within( delta ).of 6 // <6>
expect( result ).to start_with 'abc' // <7>
expect( result ).to respond_to 'login' // <8>
expect( result ).to be true // <9>
expect( result ).to be_nil // <10>
expect( result ).to be_empty // <11>
expect( result ).to exist // <12>
expect { result }.to raise_error( ErrorClass, 'message' ) // <13>
expect { result }.to throw_symbol( :symbol, 'value' ) // <14>
expect( :a => 5 ).to have_key( :a ) // <15>
expect( [3,4,5] ).to include( 5 ) // <16>
expect( [1, 2, 3] ).to contain_exactly( 2, 1, 3 ) // <17>
expect( [1, 2, 3] ).to match_array( [3, 2, 1] ) // <18>
expect( 1..10 ).to cover( 3 ) // <19>
expect { a += 3 }.to change { a }.by_at_least(2) // <20>
expect( actual ).to satisfy { |value| value == expected } // <21>
expect { actual }.to output("some output").to_stdout // <22>
expect { actual }.to output("some error").to_stderr // <23>
expect { |b| [1, 2, 3].each( &b )   }.to yield_successive_args( 1, 2, 3 ) // <24>
----
<1> Object equivalence
<2> Object identity
<3> Object comparison
<4> Expecting the result to be greater than 4 and less than 9
<5> You can use regular expressions
<6> `be_within` is useful for asserting on values that are always changing (like CPU usage)
<7> Expecting the result to start with a string
<8> If the result had a method that was name "login", this would pass
<9> Expecting result to be true
<10> If `result.nil?` returns true, then this would pass
<11> If result is an empty array, this would pass
<12> This passes if `result.exist?` or `result.exists?`
<13> Notice how this expect is a block. You are expecting the result to raise an exception.
<14> The `throw_symbol` matcher is used to specify that a block of code throws a symbol.
<15> This is useful for API result hashes to verify that an API returns the correct keys
<16> Make sure a list of values includes something you are expecting
<17> This a way to test arrays against each other in a way that disregards differences in the ordering between the actual and expected array.
<18> Same matcher as `contains_exactly`, just a different name
<19> Useful for ranges
<20> Change observation
<21> The `satisfy` matcher is extremely flexible and can handle almost anything you want to
specify
<22> Useful for making sure something is outputted to the screen
<23> Make sure an error is printed out to standard error
<24> `yield_successive_args` is designed for iterators, and will match if the method-under-test yields the same number of times as arguments passed to this matcher, and all actual yielded arguments match the expected ones using === or ==

For full documentation with many more examples see https://relishapp.com/rspec/rspec-expectations/v/3-3/docs/built-in-matchers

=== Metadata

Metadata can be specified on a *describe*, *context*, and *it* blocks at any level in the spec.  However, normally you would put all your metadata at the beginning of the spec in the first *describe*.

[source,ruby]
----
describe 'Zones',
  tags:        ['basic'], // <1>
  sys_helpers: ['zones'], // <2>
  helpers:     ['authentication'], // <3>
  loadout:     { esms: 2, recs: 3 }, // <4>
  owner:       ['First Last', 'First_Last@McAfee.com'] do // <5>

  it 'some example' do
  end
end
----
<1> *tags* can be anything you want, but the supported ones in the official test runs are 'bvt', 'basic', 'acceptance', and 'comprehensive'. You can run all tests that match a certain tag. See https://www.relishapp.com/rspec/rspec-core/v/3-3/docs/command-line/tag-option
<2> *sys_helpers* are helpers for _system tests_. They are located under _automation/helpers_. If you want to use a helper in a spec, you must specify it here first. You can add this tag at any level of your spec, including in a specific example that uses that helper. icon:video-camera[]  http://automation.ida.lab:8000/mp4/rspec_sys_helpers.mp4[Using helpers in Rspec^]
<3> *helpers* are helpers for _e2e tests_.
<4> *loadout* is where you can specify how many of each device type your test needs to run. If you only need 1 esm and 1 of each device type, you do not need to use loadouts. icon:video-camera[]  http://automation.ida.lab:8000/mp4/rspec_loadouts.mp4[Using loadouts in Rspec^]
<5> *owner* is the author of the spec, or at least the one taking responsibility for it. You can have multiple owners by specifying it as a nested array: `[['Name1', 'Email1'], ['Name2', 'Email2']]`.

video::http://automation.ida.lab:8000/mp4/rspec_tags_and_dry_run.mp4[width=500, title="Rspec tags and dry run", poster="http://automation.ida.lab:8000/thumbnails/rspec_tags_and_dry_run.jpg"]

=== Advanced RSpec structure

[source,ruby]
----
describe 'System Information' do
  before :all do
    @system_information = Helpers::SystemInformation.new
  end

  context 'Hardware' do
    let( :hardware ) { @system_information.hardware } // <1>

    context 'CPU' do // <2>
      subject { hardware[:cpu] } // <3>
      let( :expected_model ) { @esm.cmd( 'cat /proc/cpuinfo' ).chomp.strip }
      let( :expected_count ) { @esm.cmd( 'grep -c processor /proc/cpuinfo' ).chomp.to_i }

      its( [:model] ) { is_expected.to eq expected_model } // <4>
      its( [:count] ) { is_expected.to eq expected_count }
      its( [:load] )  { is_expected.to be >= 0.0 }
    end

    context 'RAM' do
      subject { hardware[:ram] }

      its( [:available] ) { is_expected.to be >= 0 }
      its( [:used] )      { is_expected.to be > 0 }
      its( [:free] )      { is_expected.to be >= 0 }
    end
  end
end
----
<1> Use `let` to set a variable that you want to use in any context and example nested below it.
<2> You can have nested context blocks to help you organize your code further.
<3> You can set the testing subject in a `subject` block. Note that it is using the variable set in the `let` block above.
<4> Because a subject was set in this context, you can refer to it implicitly. This line states (in English) that "its model is expected to equal the expected model". Who? The subject. So it could also read "The subject's model is expected...", or "The hardware CPU's model is expected...".

== Helper structure

NOTE: This section is not yet written.

== Wrapper structure

NOTE: This section is not yet written.

== Test framework niceties

NOTE: This section is not yet written.

=== Loadouts

video::http://automation.ida.lab:8000/mp4/rspec_loadouts.mp4[width=500, title="Using loadouts in Rspec", poster="http://automation.ida.lab:8000/thumbnails/rspec_loadouts.jpg"]

=== Resources

video::http://automation.ida.lab:8000/mp4/cool_things_resources.mp4[width=500, title="Resources", poster="http://automation.ida.lab:8000/thumbnails/cool_things_resources.jpg"]

=== Debug

When M^3 was used for testing, the Helper class was designed to either use the debug method to output text to the terminal window or use the log method to output text to a file. When using the spec_helper in core/automation/spec, RSpec tests also have a debug command to make switching from M^3 tests to RSpec tests easier.

To output a message to terminal using debug during a test, simply call `debug( "Message here...")`.

Unlike M^3, RSpec has "context" and "describe" blocks. The purpose of context and describe blocks are to improve readability of RSpec Tests. However, using debug inside a context or describe scope can have strange results. The debug and puts methods used in a context or describe block display their message before any test is run.

As a result, avoid using debug inside context or describe scopes. If you'd like to add a debug in your context before a test is run, call debug in a 'before' or an 'after' block.

== Writing documentation

video::http://automation.ida.lab:8000/mp4/writing_yardoc.mp4[width=500, title="Writing yardoc", poster="http://automation.ida.lab:8000/thumbnails/writing_yardoc.jpg"]

== icon:wrench[] Other helpful tools

Most of the tools below were created in-house.  You can get them by cloning the sit/misc_tools GIT repository by running `git clone git@git.ida.lab:sit/misc_tools.git`.

=== s (an SSH tool)

Tool that allows you to SSH into anything without worrying about any prompts for anything, including passwords.

----
s -h
  s [ip] [password] [username]

  Must have sshpass installed!
    sudo apt-get install sshpass

  Edit this file to set the 'default_ip', 'default_pass', and 'default_username'
    If default_ip='10.75.110.5'
      's'             would be: ssh root@10.75.110.5
      's 2'           would be: ssh root@10.75.110.2
      's 76.150'      would be: ssh root@10.75.76.150
      's 40.50.60     would be: ssh root@10.40.50.60
      's 192.168.0.5  would be: ssh root@192.168.0.5

    If default_pass='Q@_T3st.NGCP'
      's 2'              would use 'Q@_T3st.NGCP'  as the password
      's 2 Security.4u'  would use 'Security.4u' as the password

    If default_username='root'
      's 2'                would be: ssh root@10.75.110.2
      's 2 pass admin'     would be: ssh admin@10.75.110.2 with 'pass' as the password
----

video::http://automation.ida.lab:8000/mp4/helpful_tools_s.mp4[width=500, title="s", poster="http://automation.ida.lab:8000/thumbnails/helpful_tools_s.jpg"]

=== irb and irbesm

CAUTION: This section is outdated and needs to be rewritten!

Tool that allows you to use irb (interactive Ruby) with everything loaded from the @esm and @api objects.  All helpers and utilities will be added to the loadpath for you as well if you run from the "tests" directory.

video::http://automation.ida.lab:8000/mp4/helpful_tools_irb.mp4[width=500, title="irb and irbesm", poster="http://automation.ida.lab:8000/thumbnails/helpful_tools_irb.jpg"]

=== get_info

Tool to view information about a specific ESM/device (model, machine ID, login banner, device locale, build number, repository revisions, HA status, etc.) You can specify multiple IP addresses by using -i more than once or use -r to specify a range.

----
get_info -h
Usage: get_info [options]
    -i, --ip_addr IP                 IP address
    -r, --ip_range IP                IP address range (example: "2-16" or "76.100-109").
                                     Using this will only show the buildnumber and will
                                     show no results for IP's that are down or don't
                                     have a buildstamp file
    -p, --password IP                Password
    -b, --buildnumber                Only show the buildnumber
    -d, --database                   Shows the database path, DFL version, and version date
    -t, --table TABLE                Show the table version of the specified table name
    -v, --version                    Show the version of this script
    -h, --help                       Show this message
----

video::http://automation.ida.lab:8000/mp4/helpful_tools_get_info.mp4[width=500, title="get_info", poster="http://automation.ida.lab:8000/thumbnails/helpful_tools_get_info.jpg"]

=== api_collector

API Collector is a tool that shows API requests and responses on an ESM.  It has to run on an ESM.

----
perl api_collector.pl --help
###############################################################################
#  script: api_collector.pl
#
#  Input:
#    -d|debug      - None    - Optional - Print Debug Statements
#    -h|help       - None    - Optional - Prints Usage
#    -n|num        - Integer - Optional - Get N Previous Requests (+/- 1)
#    -o|oneline     - None   - Optional - Print Requests on one line
#    -i|ip         - String  - Optional - Filter by a set of IP Address's
#    -s|session    - Integer - Optional - Filter by a Session
#    -a|api        - String  - Optional - Filter by a set of API's
#    -e|ec         - String  - Optional - Filter by a set of Error Codes
#    -c|contains   - String  - Optional - Filter by a set of Strings
#    -r|raw        - None    - Optional - Print Raw Request Debug
#    -f|filter     - None    - Optional - Print Filter Debug
#
#  Usage Examples:
#    perl api_collector.pl
#    perl api_collector.pl -h
#    perl api_collector.pl -d
#    perl api_collector.pl -r
#    perl api_collector.pl -f
#    perl api_collector.pl -o
#    perl api_collector.pl -n 100
#    perl api_collector.pl -i 10.75.110.7
#    perl api_collector.pl -i 10.75.110.7 -i 10.75.110.6
#    perl api_collector.pl -a USER_LOGIN -a USER_LOGOUT
#    perl api_collector.pl -a ~QRY
#    perl api_collector.pl -a '!QRY'
#    perl api_collector.pl -e 66 -e 60
#    perl api_collector.pl -e ~0
#    perl api_collector.pl -e '!0'
#    perl api_collector.pl -c Username
#    perl api_collector.pl -c ~Username
#    perl api_collector.pl -c '!Willy'
#    perl api_collector.pl -i 10.75.110.7 -a USER_LOGIN -e '!0' -o
#    perl api_collector.pl -a QRY -a ~QRY_TERMINATE
#
#  Notes:
#    Doing a Not Error Code MUST be in single quotes (i.e. '!0').
#    May do ~ instead of ! for not.
#    API's are not printed out until the output is sent back!
#
#  Logic for Filtering:
#   -a USER -a MISC = All USER and MISC apis
#   -a USER -a MISC ~USER_LOGIN = All USER excluding USER_LOGIN and all MISC apis
#   -a ~USER -a ~QRY = Get all non USER and non QRY apis
###############################################################################
----

video::http://automation.ida.lab:8000/mp4/helpful_tools_api_collector.mp4[width=500, title="api_collector", poster="http://automation.ida.lab:8000/thumbnails/helpful_tools_api_collector.jpg"]

=== esm_tail

Outputs meaningful information about the ESM or device that it is running on and then shows messages from the log files that are of concern (known as "tailing for nastiness").  Be sure to SCP it to an ESM and then run `./esm_tail.sh`.

video::http://automation.ida.lab:8000/mp4/helpful_tools_esm_tail.mp4[width=500, title="esm_tail", poster="http://automation.ida.lab:8000/thumbnails/helpful_tools_esm_tail.jpg"]

=== byebug

https://github.com/deivid-rodriguez/byebug[Byebug] is a simple to use, feature rich debugger for Ruby 2.  It allows you to see what is going on inside a Ruby program while it executes and offers many of the traditional debugging features.

To use byebug, just add `require 'byebug'; byebug` anywhere in your Ruby file.

video::http://automation.ida.lab:8000/mp4/helpful_tools_byebug.mp4[width=500, title="byebug", poster="http://automation.ida.lab:8000/thumbnails/helpful_tools_byebug.jpg"]

== Automation system big picture

NOTE: This section is not yet written.

video::http://automation.ida.lab:8000/mp4/esm_product_overview.mp4[width=500, title="ESM product overview", poster="http://automation.ida.lab:8000/thumbnails/esm_product_overview.jpg"]

video::http://automation.ida.lab:8000/mp4/how_testing_framework_ties_in.mp4[width=500, title="The testing framework", poster="http://automation.ida.lab:8000/thumbnails/how_testing_framework_ties_in.jpg"]

video::http://automation.ida.lab:8000/mp4/automated_build_testing.mp4[width=500, title="Automated build testing", poster="http://automation.ida.lab:8000/thumbnails/automated_build_testing.jpg"]

=== Jenkins

=== Whiteboard

=== Test runner

=== icon:desktop[] Virtual machines

== Writing unit tests

=== Introduction

Unit tests are a form of tests meant to verify the correct behavior of the smallest testable unit of code - referred to hereafter as a unit. A unit typically consists of either a) a single procedure in procedural programming, or 2) a single class in object-oriented programming. A unit test seeks to ensure that the unit in question consistently behaves correctly in the presence of predetermined inputs. Unit tests differ from other tests in several ways:

* Scope: A unit test only verifies that this unit behaves as expected. It cannot tell whether the unit correctly interacts with other units. Such wider-scope tests are either integration or system tests.
* Granularity of Feedback: When a unit test fails, the developer knows exactly which unit is failing. This differs from system tests in that when a failure is detected, the offending code must be within a handful of lines, rather than having to search the entire stack as he or she would have to do when a system test fails.
* Timeliness: Unit tests are traditionally written before or at the same time as the units which they test. These tests should be available to the team as soon as the developer commits the associated production code.
* Independence: Unit tests depend on the code that they test and on nothing else. A unit test can be run in any environment that the associated production code can be compiled in. It does not require a running deployment of the production system and therefore any developer should be able to run the tests on his or her own computer.
* Speed: A correctly designed unit test suite should be able to run in a matter of seconds to minutes. This speed is key to unit tests' success. The suite must run fast enough that a developer can make a change, run the unit test suite, and know with a high degree of confidence that their change is safe to commit. If a change causes a test to fail, the developer knows immediately and does not commit his or her changes.

=== Unit Tests as Documentation

Unit tests make an excellent base for a stable project. Well written unit tests will ensure that all of the nuts and bolts of a project will reliably behave as excepted. However, the usefulness of a unit test is not limited to its use in automated testing. Well-written unit tests also serve as an excellent resource for developers seeking documentation. When a developer needs to know what a particular unit does, he or she can open the unit test associated with that unit and immediately find concise examples of the unit being used. The developer will see how the unit is expected to behave (and if the unit tests are passing, how it indeed does behave) in the presence of normal inputs. Well written tests will also illustrate how the unit behaves in edge cases, and will often also give examples of invalid input. While not a replacement for more formal documentation, using unit tests as an initial source of examples and described behaviors will save developers a lot of time. One great advantage of using unit tests as documentation is that so long as the tests are passing, the unit test cannot ever provide out-of-date information. It's the only form of documentation that is always guaranteed to be up-to-date.

=== Unit Testing Frameworks

Most modern languages -- including all languages used in the SIEM products -- have one or more unit testing frameworks written for that language. To begin writing unit tests, start with one of these premade frameworks. The frameworks all have various differences, but there are some things that are common to all:
* Test Cases: A test case represents a single scenario for a unit. For example, a class may have several methods, and a single test case may send a set of inputs to one method of an instance of that class and verify that the resulting behavior is correct.
* Assert Statements: Although not always called asserts, the concept of an assert statement is a line of test code that checks whether the state or behavior of the unit being tested is expected. A test case has one or more assert statements. All assert statements must pass for the test case to pass. Although a single test case can have multiple asserts, it should still only test a single scenario.
* Consistent, Parsable Output: A proper unit test framework will output the results of the test suite in a consistent, parsable format so that a user or an automated build environment can easily interpret the results of the test suite. This typically involves output from each test about it's success and/or failure, followed by aggregate data such as total number of tests run, total passed, total failed, etc.
* Mocking framework: It has already been mentioned that a unit test should test ONLY the unit in question and should be isolated from other units. This becomes problematic when, in real code, units often have dependencies on  and interaction with other units. If this is not accounted for, a test failure cannot be reliably attributed the code being tested. For this reason, dependencies must be mocked. The most basic way of doing this is to write stub objects that implement the dependent interface, but this can become cumbersome. Therefore, many unit test frameworks have mocking frameworks either built-in or easily pairable. These allow the test writer to quickly make a mock of an object and force known behavior for the unit's dependencies, so any failures detected by the test can reliably be attributed to the tested unit. The mock frameworks essentially allow you to create an object that can be provided to the unit as its dependency without having to write an entire subclass. Only the behavior you need is specified and everything else behaves in the default manner.

=== Best Practices

There are many ways to write unit tests, but we recommend following these best practices:
* Only unit test public functions. Anything that is private is merely implementation details
* Store unit tests with the project that the tests apply to. There are two common ways to do this: 1) have unit test files side-by-side with the units they are testing, or 2) create a unit test directory at the root of your project source. If you choose option 2, please interperet 'project' in the narrowest possible sense. For example, libmsgreader would be considered a project, not the Receiver and not the entire NitroVision repository.
* There should be no traces of test code in production code. The production code file should contain only production code.
* Unit test cases should identify normal inputs, edge-case inputs and, where possible, invalid inputs
* Use dependency injection to provide your unit with its dependencies. This can be as simple as passing all objects upon which your unit depends in the constructor rather than creating them within your constructor. This allows you to easily insert mock dependencies and thus isolate the tested unit. If this is inconvenient for how you intend to use the unit, make two constructors: one that creates the dependencies and one that accepts the dependencies. For example, if you had a class 'MyClass' which depended on class 'OtherClass', and you were in a language that requires explicit destructors, a template like the following could be used:

[source, c++]
----
class MyClass
{
  OtherClass dependency;
  bool dependencyCreated;

  public MyClass()
  {
    dependency = new OtherClass();
    dependencyCreated = true;
  }

  public MyClass(OtherClass the_dependency)
  {
    dependency = the_dependency;
    dependencyCreated = false;
  }

  public ~MyClass()
  {
    if (dependencyCreated)
    {
      dependency.free();
    }
  }
}
----

* Do not mock containers or other classes provided by the language. For example, if your unit has a HashMap or an ArrayList, do not attempt to mock these. They may be safely assumed to work correctly. If they do not, your unit test should point this out for you when it fails.
* Write all unit tests to be independent of all others. Assume the unit tests will run in a random order (which many test frameworks will do), and do not rely on a previous test to set up your state for the next test to work correctly.
* Do not unit test trivial getters and setters. If your your getters and setters contain non-trivial logic, include them in your test. If they merely set or access a variable without any validation or other logic, though, a unit test would be unnecessary.
* Unit tests should have descriptive, consistent names. Each test's name should describe what behavior it is attempting to validate. This serves to improve unit tests' function as a form of documentation, as well as make it easier for other developers to work on your unit.

=== Language-specific guidance

==== C

NOTE: This section is not yet written.

==== Java

For Java, we recommend using JUnit. A JUnit test class is a normal class containing no-argument void methods which are indicated as tests by placing the "@Test" annotation immediately before the method. Tests are executed using the org.junit.runner.JUnitCor class, and results are stored in org.junit.runner.Result. Depending on your IDE, running the tests can be automatically set up for you. Please see section 5 of the following tutorial:

http://www.vogella.com/tutorials/JUnit/article.html#junittesting

==== Javascript

For javascript. we recommend the Jasmine test framework. Its syntax is extremely similar to RSpec. The framework consists of "describe" blocks which represent a suite of tests, each with one or more "it" blocks that test a specific piece of the unit. For example, if I had a unit of code (a function or object) that I was testing called "MyUnit", I would Create a describe block as follows:

[source, javascript]
----
describe("MyUnit", function(){});
----

This is an empty test suite. In order to add tests to it, we would put one or more "it" blocks inside the empty anonymous function, as follows:

[source, javascript]
----
describe("MyUnit", function(){
  it("can multiply two positive numbers", function(){
    answer = MyUnit.multiply(2,3);
    expect(answer).toBe(6);
  })
})
----

The above test makes sure that the function MyUnit.juggle() returns true. Note that the "excpect" declarations are what verifies that the test was successful. Also, note the way this code reads as a form of documentation. The test seeks to describe the behavior of the unit (hence the text suite is declared with a "describe" block). It then makes one or more statements about how this unit behaves: "It can multiply two positive numbers" or "It rejects negative values", or any other way that this unit is expected to behave.

To learn more about how to use Jasmine, please visit the following link:
http://jasmine.github.io/2.3/introduction.html

==== FreePascal

For FreePascal, we recommend fpUnit. Unit testing in fpUnit is accomplished by creating a subclass of the TTestCase Class. Each no-argument procedure in the class's published section is considered a test to be run. Within each test procedure, one or more Check functions are executed (these are the assert statements from most test frameworks). There are many Check functions that cover a wide range of scenarios. CheckEquals, CheckGreaterThan, CheckNotNull, etc. are all provided as part of the framework. They all take the actual value (output of your tested unit), an expected value, where that makes sense, and an optional message to display in case the check fails. The test class is registered with the framework by executing the following line:

TestFramework.RegisterTest(MyTestCase.Suite);

Then all registered tests are run by executing the following:

[source, pascal]
----
RunRegisteredTests();
----

The simplest way to implement this would be to create a program where the main file includes all test class files in its uses statement, and putting the line that registers the test class in the initialization block of the test class's unit. This way, when the program runs, it will run all of the initialization blocks, thus registering all of the tests, and then run all tests. This approach does not allow the developer to only run a subset of the tests, though. More clever methods are needed for that. Such clever methods are outside the scope of this article.

Below is a simple example of unit testing MyUnit in Freepascal:

===== Original Unit

[source, pascal]
----
unit MyUnit;

interface

type
  TMyClass = class
    public
      function multiply(a, b : integer) : boolean;
  end;

implementation

function TMyClass.multiply(a, b : integer) : boolean;
begin
  result := a * b;
end;
----

===== Unit Test
[source, pascal]
----
unit MyUnit_test;

interface

uses
  MyUnit,
  TestFramework;

type
  TMyUnitTestCase = class(TTestCase)
    published
      procedure TestMyClassCanMultiplyPositives;
  end;

implementation

procedure TMyUnitTestCase.TestMyClassCanMultiplyPositives;
var
  answer     : boolean;
  myInstance : TMyClass;

begin
  myInstance := TMyClass.Create();
  try
    answer := myInstance.multiply(2, 3);
    CheckEquals(6, answer, 'Failed to multiply positive integers');
  finally
    myInstance.Free();
  end;
end;

initialization
  TestFramework.RegisterTest(TMyUnitTestCase.Suite);
end;
----

===== Test Runner Program
[source, pascal]
----
program RunTests;

uses
  Classes,
  MyUnit_test,
  TextTestRunner;
begin
  RunRegisteredTests;
end.
----

==== Perl


NOTE: This section is not yet written.

== Writing system tests

NOTE: This section is not yet written.

== Writing e2e tests

NOTE: This section is not yet written.

NOTE: View documentation at https://bugzilla.ida.lab/wiki/index.php/End-to-End_GUI_Testing

video::http://automation.ida.lab:8000/mp4/gui_testing_intro.mp4[width=500, title="Introduction", poster="http://automation.ida.lab:8000/thumbnails/gui_testing_intro.jpg"]

video::http://automation.ida.lab:8000/mp4/end_to_end_intro.mp4[width=500, title="Intro to End-to-End tests", poster="http://automation.ida.lab:8000/thumbnails/end_to_end_intro.jpg"]

video::http://automation.ida.lab:8000/mp4/end_to_end_helpers.mp4[width=500, title="Using helpers in End-to-End tests", poster="http://automation.ida.lab:8000/thumbnails/end_to_end_helpers.jpg"]

video::http://automation.ida.lab:8000/mp4/end_to_end_faker.mp4[width=500, title="Using faker in End-to-End tests", poster="http://automation.ida.lab:8000/thumbnails/end_to_end_faker.jpg"]

== Jobs

NOTE: Detailed documentation for jobs can be found at http://automation.ida.lab:1234/docs/siem_api/SiemApi/Job

=== Calling a job

An instance of the job class is returned when an api method starts with 'OpCode_' and maps to a currently defined job code in jobs_opcodes.rb.

[source,ruby]
----
@elm = @esm[:elm].first
job = @esm.OpCode_ELMCreatePool( @elm.ipsid, "Name=#{pool_name}\nDesc=#{pool_desc}" )
job.wait # blocks execution until job completes.
debug job.running?
debug job.finished?
debug job.jec
debug job.response # Converts result[:resp] name/value pairs string to a hash.
debug job.result[:resp] # Get the raw response
assert_job_call( job.result, ERROR_Ok )
----

Note: `#response` will attempt to parse the result into a hash, but some Jobs may not return results that map correctly. Your mileage may vary.

=== Calling a job with a params hash

In addition to taking a static params string, the Job runner also takes a params hash that would look like this:

[source,ruby]
----
@elm = @esm[:elm].first
job = @esm.OpCode_ELMCreatePool( @elm.ipsid, { Name: pool_name, Desc: pool_desc } ).wait
assert_job_call( job.result, ERROR_Ok )
----

Note: Not all job inputs map cleanly to name/value pairs so there may be times where the parameter string is the best choice.

=== Create a job class from a job id

An alternate constructor has been added which allows a job class to be created using just a job id. This is especially useful for jobs that are kicked off from an API call.

[source,ruby]
----
@elm = @esm[:elm].first
job = @esm.OpCode_ELMCreatePool( @elm.ipsid, "Name=#{pool_name}\nDesc=#{pool_desc}" )
job2 = @esm.job_from_id( job.id )

# if an invalid job_id is passed to new_from_id, an exception will be raised

job2.wait # Use it the same way as the original job class.
debug job2.response
----

=== Call a job from a helper

When calling a job from a helper, you typically want the job error code (JEC) to be 0 at all times. Rather than have to check to make sure that job error code (JEC) is 0 every time you call a job within a helper, you can append a bang (!) to the job name and if job error code (JEC) is not 0, it will throw an exception.

[source,ruby]
----
@esm.OpCode_SomeJob!( @esm.ipsid )
----

Calling job_from_id! will have the same behavior:

[source,ruby]
----
@esm.job_from_id!( result[:jid] )
----
